{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sunita Inderjit\n",
        "\n",
        "NLTK Project"
      ],
      "metadata": {
        "id": "91B2Kd1_ZzYo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rz2d9GtMZuny"
      },
      "outputs": [],
      "source": [
        "# Import necessary nltk library\n",
        "\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk.download('stopwords') Predefined set of common words (like \"the\", \"is\", \"in\") from the NLTK library,\n",
        "which are typically filtered out in text processing because they carry less meaningful information\n",
        "for tasks like classification or sentiment analysis"
      ],
      "metadata": {
        "id": "f0T4qzzhd5zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5UOeMeGajTP",
        "outputId": "51467e57-7a81-4bb5-9e1d-cc25df2cd06d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OuDhkBTCaw9Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88124ed9"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "\n",
        "Tokenization  is the process of splitting text into smaller units called \"tokens,\" which could be words, phrases, or even characters. It helps in breaking down raw text for further processing, like analysis or transformation. There are two main types of tokenization:\n",
        "\n",
        "**Word Tokenization:** Splits the text into individual words.\n",
        "Example: \"I love NLP\" becomes ['I', 'love', 'NLP']\n",
        "\n",
        "**Sentence Tokenization:** Splits the text into individual sentences.\n",
        "Example: \"Hello world. I love NLP!\" becomes ['Hello world.', 'I love NLP!']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3avoYM39avlM",
        "outputId": "19ebd659-6f1d-4683-84ea-199e9476c7af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = 'Sunita good at learning natural language processing'\n",
        "word_tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGPSWltYaZby",
        "outputId": "bcbe51a9-f10e-4cc2-e07d-dff1b892474d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sunita', 'good', 'at', 'learning', 'natural', 'language', 'processing']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = 'Sunita good at learning natural language processing'\n",
        "word_tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unbSoqBJbEnf",
        "outputId": "bbdb2f68-8ce0-4d7b-9f48-ade31084df3f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sunita', 'good', 'at', 'learning', 'natural', 'language', 'processing']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Don't worry if you don't know everything perfectly, just keep trying because no one is perfect.\"\n",
        "word_tokenize(s.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3YxtmjgbEkG",
        "outputId": "d1409fd5-c83d-4d0e-adfd-27ae57b4643b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['do',\n",
              " \"n't\",\n",
              " 'worry',\n",
              " 'if',\n",
              " 'you',\n",
              " 'do',\n",
              " \"n't\",\n",
              " 'know',\n",
              " 'everything',\n",
              " 'perfectly',\n",
              " ',',\n",
              " 'just',\n",
              " 'keep',\n",
              " 'trying',\n",
              " 'because',\n",
              " 'no',\n",
              " 'one',\n",
              " 'is',\n",
              " 'perfect',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"\"\"just finished learned word tokenization.\n",
        "Next  will learn sentence tokenization.\n",
        "There are many options for tokenization using nltk\"\"\"\n",
        "\n",
        "word_tokenize(s.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VChS4VnRbEhR",
        "outputId": "dbe81bbe-3a10-40c5-c20e-e634d1838c92"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['just',\n",
              " 'finished',\n",
              " 'learned',\n",
              " 'word',\n",
              " 'tokenization',\n",
              " '.',\n",
              " 'next',\n",
              " 'will',\n",
              " 'learn',\n",
              " 'sentence',\n",
              " 'tokenization',\n",
              " '.',\n",
              " 'there',\n",
              " 'are',\n",
              " 'many',\n",
              " 'options',\n",
              " 'for',\n",
              " 'tokenization',\n",
              " 'using',\n",
              " 'nltk']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the WordPunctTokenizer from nltk\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "# create an instance of the tokenizer\n",
        "tokenizer = WordPunctTokenizer()"
      ],
      "metadata": {
        "id": "dTPU_O0WbEc0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of tokenizer\n",
        "s = \"You are unstoppable keep going\"\n",
        "\n",
        "# every word in the sentence is split\n",
        "\n",
        "tokenizer.tokenize(s.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRHLP_PybVEc",
        "outputId": "97677254-58d8-4228-84a3-3108e5c43b9e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you', 'are', 'unstoppable', 'keep', 'going']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Tokenization"
      ],
      "metadata": {
        "id": "8gxZCYnFbZmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "PXtwCk_6bVBU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"\"\"I am having fun learning word tokenization.\n",
        "now time to learn sentence tokenization.\n",
        "Also many options avaailable for tokenization using nltk\"\"\"\n",
        "\n",
        "sent_tokenize(s.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7k-uhmQbU97",
        "outputId": "c6c0d266-4608-4d87-cfdd-a12aa47f8703"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i am having fun learning word tokenization.',\n",
              " 'now time to learn sentence tokenization.',\n",
              " 'also many options avaailable for tokenization using nltk']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering Stopwords"
      ],
      "metadata": {
        "id": "JRBAn_gBbnLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQJ8S8Hqbjq4",
        "outputId": "b0c57d3e-bad3-41ef-a259-287288339d76"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNdBmaBGbjfa",
        "outputId": "38ac0492-a2c9-4dc8-87f6-46f5d9c22a0c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = 'This is a simple sentence for demonstrating stop word filtering in NLP'\n",
        "filtered_words = []\n",
        "for word in s.split():\n",
        "    if word not in stop_words:\n",
        "        filtered_words.append(word)\n",
        "\n",
        "' '.join(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1ZX-Lmw2bjb5",
        "outputId": "32b41d26-d457-4912-923d-b0ce730edb47"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This simple sentence demonstrating stop word filtering NLP'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_words = [word for word in s.split() if word not in stop_words]\n",
        "' '.join(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6Li6_5pIb16M",
        "outputId": "536fb1db-6cc9-4050-d86d-85fcd8461c48"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This simple sentence demonstrating stop word filtering NLP'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "Stemming is a text normalization technique in Natural Language Processing (NLP) where words are reduced to their base or root form by removing prefixes or suffixes. The stemmed word may not always be a valid word, but it helps to group similar words with the same root, improving the efficiency of text processing tasks such as search engines, information retrieval, or sentiment analysis.\n",
        "\n",
        "For example:\n",
        "\n",
        "\"running\", \"runner\", and \"ran\" are all reduced to \"run\".\n",
        "\"connected\", \"connection\", and \"connecting\" will be reduced to \"connect\".\n"
      ],
      "metadata": {
        "id": "uxqzNZ11b61z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmer.stem('breaking')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3cXpY_6rb13Y",
        "outputId": "34cd4629-963b-46e0-f97c-e7f3c41a2774"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'break'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem('Writing')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Zshv2ioIb10M",
        "outputId": "6e9bc5c6-4c8a-4754-f27b-8e9d4029d2cb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'write'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem('troubling')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o3_bXZIFb1tK",
        "outputId": "ca7ff056-f849-46c7-a8fb-15265b01cf48"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'troubl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmer.stem('breaks'))\n",
        "print(stemmer.stem('breaking'))\n",
        "print(stemmer.stem('broke'))\n",
        "print(stemmer.stem('broken'))\n",
        "print(stemmer.stem('changes'))\n",
        "print(stemmer.stem('changed'))\n",
        "print(stemmer.stem('changing'))\n",
        "print(stemmer.stem('writes'))\n",
        "print(stemmer.stem('writing'))\n",
        "print(stemmer.stem('wrote'))\n",
        "print(stemmer.stem('running'))\n",
        "print(stemmer.stem('ran'))\n",
        "print(stemmer.stem('run'))\n",
        "print(stemmer.stem('trouble'))\n",
        "print(stemmer.stem('troubling'))\n",
        "print(stemmer.stem('troubles'))\n",
        "print(stemmer.stem('troubled'))\n",
        "print(stemmer.stem('cats'))\n",
        "print(stemmer.stem('knives'))\n",
        "print(stemmer.stem('leaves'))\n",
        "print(stemmer.stem('jumping'))\n",
        "print(stemmer.stem('jumped'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhA4hF7Jb1qC",
        "outputId": "4d7d9952-0094-45c0-f8f9-963a9f75161a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "break\n",
            "break\n",
            "broke\n",
            "broken\n",
            "chang\n",
            "chang\n",
            "chang\n",
            "write\n",
            "write\n",
            "wrote\n",
            "run\n",
            "ran\n",
            "run\n",
            "troubl\n",
            "troubl\n",
            "troubl\n",
            "troubl\n",
            "cat\n",
            "knive\n",
            "leav\n",
            "jump\n",
            "jump\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"\"\"We've covered word tokenization and how to filter stopwords. Now,\n",
        "let's explore sentence tokenization, where each sentence is treated as a single unit\n",
        "instead of breaking it down into words. Learning natural\n",
        "language processing can be challenging at times, but it's definitely fun!.\n",
        "\"\"\"\n",
        "\n",
        "stem_words = [stemmer.stem(word) for word in s.split()]\n",
        "stem_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuRroRjYcL4q",
        "outputId": "abf015ff-52c2-4201-9eb2-311477dcc657"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"we'v\",\n",
              " 'cover',\n",
              " 'word',\n",
              " 'token',\n",
              " 'and',\n",
              " 'how',\n",
              " 'to',\n",
              " 'filter',\n",
              " 'stopwords.',\n",
              " 'now,',\n",
              " \"let'\",\n",
              " 'explor',\n",
              " 'sentenc',\n",
              " 'tokenization,',\n",
              " 'where',\n",
              " 'each',\n",
              " 'sentenc',\n",
              " 'is',\n",
              " 'treat',\n",
              " 'as',\n",
              " 'a',\n",
              " 'singl',\n",
              " 'unit',\n",
              " 'instead',\n",
              " 'of',\n",
              " 'break',\n",
              " 'it',\n",
              " 'down',\n",
              " 'into',\n",
              " 'words.',\n",
              " 'learn',\n",
              " 'natur',\n",
              " 'languag',\n",
              " 'process',\n",
              " 'can',\n",
              " 'be',\n",
              " 'challeng',\n",
              " 'at',\n",
              " 'times,',\n",
              " 'but',\n",
              " \"it'\",\n",
              " 'definit',\n",
              " 'fun!.']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatization is a process in Natural Language Processing (NLP) that reduces a word to its base or dictionary form, known as the lemma. Unlike stemming, lemmatization considers the context and the actual meaning of the word, which helps to ensure that the base form is a valid word.\n",
        "\n",
        "For example:\n",
        "\n",
        "The words \"running\" and \"ran\" would be reduced to \"run.\"\n",
        "The words \"better\" and \"good\" would be reduced to \"good.\"\n",
        "Lemmatization takes into account the part of speech (POS) of a word to apply more accurate transformations, making it more sophisticated than stemming."
      ],
      "metadata": {
        "id": "LGnCyDvWcQDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycW7nXuvcL1z",
        "outputId": "40576c4b-e2ee-4940-8fca-9074e2460c2d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('breaks')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "19F-ldDqcLy7",
        "outputId": "16269533-746c-4c58-9b35-675036461c22"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'break'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('break', pos = 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UcF8bNMecLwD",
        "outputId": "141c6649-ac7f-4ed1-adeb-ecdef7420e36"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'break'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize('breaks',  pos = 'v'))\n",
        "print(lemmatizer.lemmatize('breaking',  pos = 'v'))\n",
        "print(lemmatizer.lemmatize('broke',  pos = 'v'))\n",
        "print(lemmatizer.lemmatize('broken',  pos = 'v'))\n",
        "print(lemmatizer.lemmatize('changes',  pos = 'v'))\n",
        "print(lemmatizer.lemmatize('changed', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('changing', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('writes', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('writing', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('wrote', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('running', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('ran', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('run', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('trouble', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('troubling', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('troubles', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('troubled', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('cats', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('knives'))\n",
        "print(lemmatizer.lemmatize('leaves'))\n",
        "print(lemmatizer.lemmatize('jumping', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('jumped', pos = 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE5TAGbicLla",
        "outputId": "e3ef8466-34ce-4978-ca0f-4b6297856d36"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "break\n",
            "break\n",
            "break\n",
            "break\n",
            "change\n",
            "change\n",
            "change\n",
            "write\n",
            "write\n",
            "write\n",
            "run\n",
            "run\n",
            "run\n",
            "trouble\n",
            "trouble\n",
            "trouble\n",
            "trouble\n",
            "cat\n",
            "knife\n",
            "leaf\n",
            "jump\n",
            "jump\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1174cd9"
      },
      "source": [
        "## POS tagging   (part of speech tagging)\n",
        "\n",
        "Part of Speech (POS) tagging is the process of labeling each word in a sentence with its corresponding part of speech, such as noun, verb, adjective, etc. This is an essential step in many Natural Language Processing (NLP) tasks as it helps to understand the grammatical structure of the sentence and the role of each word.\n",
        "\n",
        "For example, in the sentence \"The cat is sitting on the mat,\" POS tagging would assign the following tags:\n",
        "\n",
        "The → Determiner (DT)\n",
        "cat → Noun (NN)\n",
        "is → Verb (VBZ)\n",
        "sitting → Verb (VBG)\n",
        "on → Preposition (IN)\n",
        "the → Determiner (DT)\n",
        "mat → Noun (NN)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJpFFBEZcgk0",
        "outputId": "9b8fef6a-0003-4846-ab2e-0659556bb7cd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8ccb7961",
        "outputId": "f4db6cd6-e2bc-4112-a3b1-9a831749d596",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['He', 'is', 'running']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "s = 'He is running'\n",
        "words = s.split()\n",
        "words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "pos_tag(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdjk5g06cgej",
        "outputId": "973b8c3a-c107-4f8a-8395-2e5c4446f668"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('He', 'PRP'), ('is', 'VBZ'), ('running', 'VBG')]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tags\n",
        "- CC\tcoordinating conjunction\n",
        "- CD\tcardinal digit\n",
        "- DT\tdeterminer\n",
        "- EX\texistential there\n",
        "- FW\tforeign word\n",
        "- IN\tpreposition/subordinating conjunction\n",
        "- JJ\tThis NLTK POS Tag is an adjective (large)\n",
        "- JJR\tadjective, comparative (larger)\n",
        "- JJS\tadjective, superlative (largest)\n",
        "- LS\tlist market\n",
        "- MD\tmodal (could, will)\n",
        "- NN\tnoun, singular (cat, tree)\n",
        "- NNS\tnoun plural (desks)\n",
        "- NNP\tproper noun, singular (sarah)\n",
        "- NNPS\tproper noun, plural (indians or americans)\n",
        "- PDT\tpredeterminer (all, both, half)\n",
        "- POS\tpossessive ending (parent\\ ‘s)\n",
        "- PRP\tpersonal pronoun (hers, herself, him, himself)\n",
        "- RB\tadverb (occasionally, swiftly)\n",
        "- RBR\tadverb, comparative (greater)\n",
        "- RBS\tadverb, superlative (biggest)\n",
        "- RP\tparticle (about)\n",
        "- TO\tinfinite marker (to)\n",
        "- UH\tinterjection (goodbye)\n",
        "- VB\tverb (ask)\n",
        "- VBG\tverb gerund (judging)\n",
        "- VBD\tverb past tense (pleaded)\n",
        "- VBN\tverb past participle (reunified)\n",
        "- VBP\tverb, present tense not 3rd person singular(wrap)\n",
        "- VBZ\tverb, present tense with 3rd person singular (bases)\n",
        "- WDT\twh-determiner (that, what)\n",
        "- WP\twh- pronoun (who)\n",
        "- WRB\twh- adverb (how)\n",
        "- PRP$\tpossessive pronoun (her, his, mine, my, our )"
      ],
      "metadata": {
        "id": "WXVoXqjEc7aU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"\"\"There were three friends and they were wearing scarves,\n",
        "they were shivering in the snow,\n",
        "they were singing a song as it was one of there hobbies\"\"\"\n",
        "\n",
        "words = word_tokenize(s)\n",
        "nltk.pos_tag(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdkA3jmPdAlq",
        "outputId": "e68ad5d6-8863-49af-acde-0e89d2c05477"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('There', 'EX'),\n",
              " ('were', 'VBD'),\n",
              " ('three', 'CD'),\n",
              " ('friends', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('they', 'PRP'),\n",
              " ('were', 'VBD'),\n",
              " ('wearing', 'VBG'),\n",
              " ('scarves', 'NNS'),\n",
              " (',', ','),\n",
              " ('they', 'PRP'),\n",
              " ('were', 'VBD'),\n",
              " ('shivering', 'VBG'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('snow', 'NN'),\n",
              " (',', ','),\n",
              " ('they', 'PRP'),\n",
              " ('were', 'VBD'),\n",
              " ('singing', 'VBG'),\n",
              " ('a', 'DT'),\n",
              " ('song', 'NN'),\n",
              " ('as', 'IN'),\n",
              " ('it', 'PRP'),\n",
              " ('was', 'VBD'),\n",
              " ('one', 'CD'),\n",
              " ('of', 'IN'),\n",
              " ('there', 'EX'),\n",
              " ('hobbies', 'NNS')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mini NLP project"
      ],
      "metadata": {
        "id": "qPi8syKjpTbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Sunita you are unstoppable!\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D6eYEnrdAiq",
        "outputId": "c6347a67-d868-4dc9-8d4b-463932db3e32"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sunita', 'you', 'are', 'unstoppable', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s_BB7j3nN2w",
        "outputId": "ca7ddc72-9c48-4890-8185-2fb23a46c129"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sunita', 'unstoppable', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Poivc3_tnNz4",
        "outputId": "2b247577-3afc-4508-8897-c8bd8fe65ef5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sunita', 'unstopp', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word, pos='v')\n",
        " for word in filtered_tokens]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EXMrQTQnNww",
        "outputId": "88abb012-4b5a-43ef-b9c2-6905bb8f891e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sunita', 'unstoppable', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos_tags = nltk.pos_tag(lemmatized_tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwwZWEH8nNtp",
        "outputId": "4eee6a42-f701-4ae2-a705-6ca642dec289"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Sunita', 'NNP'), ('unstoppable', 'JJ'), ('!', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ggto3TPWnNqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c7tUXaLPcLhu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}